\documentclass{article}
\usepackage[backend=biber, citestyle=authoryear, bibencoding=utf8]{biblatex}
\addbibresource{./bibs/vlab-report.bib}
\addbibresource{./bibs/unicef-ie.bib}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{lscape}
\usepackage{csquotes}
\usepackage{float}
\usepackage{amsmath}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{dcolumn}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=10mm,
 top=10mm
 }

\title{Bebbo}
\date{September 2023}

\begin{document}

\maketitle

\tableofcontents

\clearpage
\section{Executive Summary}

To support parents to receive timely and quality guidance even when direct contact with service  providers is not possible and overcome barriers in access to localized digital solutions with verified  content, UNICEF Europe and Central Asia Regional Office (ECARO) developed a mobile parenting app,  Bebbo. The mobile application also supports the most vulnerable parents/caregivers with lower  education level, in terms of the navigation modalities, off-line operability and selection of the core  content. The two main objectives of Bebbo, in line with the UNICEF ECARO Early Childhood  Development Theory of Change, are: (1) Improving availability of information for parents on child  development, and (2) Supporting parents for responsive caregiving and early intervention.  Accordingly, Bebbo app provides users information and interactive tools to help nurture and aid their  child’s health and development. The launch of Bebbo in 11 countries in the ECA region is a direct  response to the identified objective to engage parents and caregivers in nurturing care, positive  parenting, stimulating, and learning.

\subsection*{The Context}
Parents everywhere are in need of information on various aspects of child development from reliable  and validated sources as well as guidance on how to support the health and development of their  children. However, services providing this sort of information and support are often non-existent or  inaccessible for a lot of parents in many places. Often, service providers, even when accessible, might  lack necessary knowledge and skills to respond to the questions and concerns parents might have.

Mobile apps are one of the most convenient and easy ways to access information about child  development and parenting. However, parenting apps are mainly in English and provide a limited  thematic content without a possibility for parents to familiarize with, track, and support all aspects of  their child’s health and development. In addition, these apps are, naturally, not adapted to contexts of  individual countries. Many apps are not free of charge, which presents a significant barrier, particularly  for the most vulnerable families. At the same time, the majority of the existing apps operate only in  online mode requiring good internet connectivity that is lacking in remote and rural areas.

\subsection*{The App}

[UNICEF TEAM TO PROVIDE MORE INFORMATION ABOUT THE APP IF DESIRED]

\subsection*{Impact Evaluation}

We perform a study across two countries, Serbia and Bulgaria, using a randomized encouragement design to compare the impact of encouraging caregivers of young children to use the Bebbo app as compared to a treatment-as-usual (TAU) condition of encouraging them to use a static informational website. By comparing Bebbo to the existing TAU, we are asking the question: “does this new treatment offer something above and beyond the already existing treatments which parents might presumably already be asked to do?”

We measure effects on eight outcomes across three domains: knowledge, attitudes, and practices. A difference-in-difference design is used, with questions asked at both the baseline (before treatment) and the endline (at least 4 weeks after treatment) surveys. Finally, an additional follow-up survey was sent (at least 4 weeks after the endline), to measure impacts of longer-term usage.

\subsection*{Results}

We do not find evidence that asking this population to use Bebbo has any impact beyond that of asking them to visit a parenting website. Given the study design, the following facts may have contributed to the lack of evidence of impact:

\begin{enumerate}
\item The population was already very “good” in regards to the outcomes of interest. We measured the improvement of parents over time, under treatment, but many could not improve from their baseline scores, which were perfect. This implies that either (i) the outcome questions were not the right questions or (ii) the problem being solved only exists in a subset of the population and measuring the impact on the population as a whole might be difficult.
\item Participants improved from the first questionnaire to the second questionnaire, regardless of treatment arm and regardless of compliance. This seems to imply that the very act of asking the questions improves the way that parents answer them. This implies that reminding parents about the questions, via an informational campaign, may be more important than providing resources to find the answers, which they seem to already have.
\item Very few people complied with treatment and used Bebbo. Of those who were asked to use the app, 28\% used the app, 12\% used the app more than one day, and only 3\% used the app more than three days. Significant pre-exposure to the app in both countries (55\% knew about the app and 23\% reported having used it before) could have led to the low initial compliance. This implies that the app may only be an effective intervention for a small subset of the population who finds it engaging. [ADD SENTENCE ABOUT APP USAGE ANALYSIS].
\end{enumerate}

We were reasonably powered (70\%) to find a small effect size on the population or a medium effect size on the treated with a 28\% takeup and no effect on those who don't take up the treatment. Note, this implies that we were not able to detect a smaller effect size or a medium effect size on a smaller takeup group (i.e. the small group of treated users who used the app for multiple days).

\section{Evaluation Questions}

\addcontentsline{toc}{subsection}{What question does this evaluation answer?}
\subsection*{What question does this evaluation answer?}
The design of the study is set up to answer the following question in the positive:

\begin{displayquote}
Is asking parents to use Bebbo an effective policy to improve the parenting knowledge, attitudes, and practices of the general population of caregivers of young children in Bulgaria and Serbia?
\end{displayquote}

\noindent Note that the study cannot fully answer the question in the negative, it cannot prove that this intervention is ineffective, it can only fail to measure its effectiveness.

We are only testing the effectivess of ``asking'' or ``inviting'' parents to use the app. Alternatively, one might be interested in testing the effectiveness ``incentivizing'' or ``forcing'' parents to use the app, but we are not doing that in this evaluation. We consider ``an effective policy'' one which performs better than the ``treatment as usual'' case, which we will consider to be an existing, static website. Finally, we are studying the general population of caregivers of young children. No particular care was given to single out any particular subset of the population that might benefit the most (or the least) from Bebbo, nor those who would be most likely to use Bebbo.


\section{Study Design}

\addcontentsline{toc}{subsection}{Experiment Design}
\subsection*{Experiment Design}

This study follows a prepost design (\cite{Clifford2021}) in which we measure the outcomes of interest before treatment (in a baseline survey) and after treatment (in an endline survey). We add an additional survey after the endline, referred to as a follow up, to look for longer-term impacts and test the impact of continued app usage.


% \vspace{1cm}
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/design-timeline.png}
\caption{Study Design}
\label{fig:Study Design}
\end{figure}

\noindent Study participants are randomized, from the beginning, to one of two conditions:

\begin{enumerate}
\item \textbf{Treatment.} Participants in the treatment condition were told that there was one more step to qualify for the study and were then asked to download the app Bebbo and use it regularly, being encouraged that doing so will help them with their parenting.
\item \textbf{Control.} Participants in the control condition were told that there was one more step to qualify for the study and were then asked to visit a parenting website and use it regularly, being encouraged that doing so will help them with their parenting.
\end{enumerate}

\noindent This follows a randomized encouragement design (\cite{Moayyedi2014}), as participants were asked to participate in the treatment, but it was not forced, thus leading to takeup that is less than 100\%. A randomized encouragement design is used here because:

\begin{enumerate}
\item We are interested in the impact of a treatment on a population where individuals can choose whether or not to take the treatment (the “compliers”).
\item The compliers and non-compliers might have different reactions to the treatment.
\end{enumerate}




\addcontentsline{toc}{subsection}{Treatment Condition}
\subsection*{Treatment Condition}

Participants were sent the following message at the end of the baseline survey:

\begin{quote}
There is just one more step to qualify for the Visa gift card of X. Please download Bebbo, the free parenting app, and discover how it can help you. Using Bebbo regularly can improve your interactions with your children and help you support their development better! You can do so by clicking the link below:
\end{quote}

Clicking on the link led them to the Bebbo app page where they were invited to download the app via the app stores (Google or Apple). App usage in the treatment group was tracked via tracking ids sent with the link to the app download page, allowing us to follow the app usage of each individual treatment participant and measure takeup. If someone decided to ignore the page, and instead went on their own to search for and download Bebbo, we would not have data on their usage. Thus, usage data and takeup should be considered a lower bound.

[UNICEF TO ADD MORE INFORMATION ABOUT THE APP AND HOW IT WORKS]

We collected all app usage events. For the sake of our study, we were interested in a subset of events that represented the accessing of content or features that contained information or might impact their behavior.


\addcontentsline{toc}{subsection}{Control Condition}
\subsection*{Control Condition}

Participants were sent the following message at the end of the baseline survey:

\begin{quote}

There is just one more step to qualify for the Visa gift card of X. Please visit the following free parenting website and discover how it can help you. Using this website regularly  your interactions with your children and help you support their development better! You can do so by clicking the link below:
\end{quote}

The choice to use a website as a treatment-as-usual (TAU) condition was decided by the evaluation and program team because it represented an alternative (and traditional/existing) way to solve the problem that the Bebbo app was trying to solve. Another option that was considered was to use an alternate parenting app, but the team believed that using a website gave the best chance to detect a difference in the use of an app, rather than the specific implementation of the Bebbo app. Similarly, several websites were considered, and the most basic website was chosen so as to be ``static'' - without interactive features - so that it acted as an informational resource rather than a web app or platform which would similarly overlap with the concepts behind the Bebbo app.

The website chosen or Bulgaria was 9meseca.bg. Due to a mistake in the implementation, no website was chosen for Serbia and the Bulgarian website was sent to participants in both countries. This implies that for Bulgaria, participants were provided a reasonable alternative to the app. However, in Serbia, they were sent a Bulgarian website, which would be expected to be suboptimal for the particpants. Similarity of results across countries shows that the choice of the treatment-as-usual website did not materially affect results, as discussed in the sections on results.


\addcontentsline{toc}{subsection}{Recruitment}
\subsection*{Recruitment}

Participants were recruited to the study with social media ads on the Meta platform (Facebook and Instagram) using the Virtual Lab platform to create and run the recruitment ads. The Virtual Lab platform is used to track and measure the price-per-respondent across multiple strata, solving the core problem of monitoring, computing expectations, and adjusting budget when recruiting via social media platforms. In this study, recruitment was not stratified, due to initial budget pressures when stratifying in the initial pilot.

In exchange for participating in the study, participants were told they could receive gift cards worth up to 12 USD (in their local currency). See figure \ref{fig:Recruitment Ads} for examples of the ad material used for recruiting. Recruitment and survey administration was performed on a rolling basis between March and October, 2023. Each individual participant was treated at the end of the baseline survey and sent the endline survey 4 weeks after completing the baseline survey.

The survey was administered via a chatbot in Facebook Messenger, using the Virtual Lab platform. Respondents who clicked on the advertisements were directed to a Messenger chat with the Virtual Lab Facebook page, which did not contain any content or information related to this study. Consent was provided via chat, as well as all answers to the survey questions and the treatment condition. Gift cards were also provided via chat, using the Tremendous gift card platform to provide Visa international prepaid cards. The Virtual Lab chatbot allowed the researchers to create multi-wave surveys, with independent timing. It additionally allowed the easy provision of gift cards at the end of each wave, which is integrated into the survey directly via the platform.

[TODO: add recruitment stats]


\begin{figure}[H]
\centering
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=100px]{images/recruitment/558.png}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=100px]{images/recruitment/639.png}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=100px]{images/recruitment/742.png}
\end{subfigure}
\caption{Recruitment Ads}
\label{fig:Recruitment Ads}
\end{figure}


\section{Descriptives}


\addcontentsline{toc}{subsection}{Respondent Characteristics by Country}
\subsection*{Respondent Characteristics by Country}

Table \ref{tbl:Baseline Respondent Characteristics} provides the baseline characteristics of the respondent population, separated by country.

Generally speaking, most respondents were themselves parents (not grandparents or other caregives), women, under 35 years of age, and spoke the dominant language of the country at home. A little over half had children 0-2, compared to 2-6 years of age. Respondents in Bulgaria were more likely to have a university education (42\%) compared to those in Serbia (29\%).

\input{descriptives/tables/Baseline Respondent Characteristics}

\addcontentsline{toc}{subsection}{Construct Variables}
\subsection*{Construct Variables}

The outcomes of interest consist of eight constructs divided into three domains: knowledge and awareness, confidence and attitudes, and practices. The mapping between the constructs, domains, and questions that make up the constructs are laid out in table \ref{tbl:Construct Variable Mapping}.

The constructs ``Vaccine Knowledge'', ``Parenting Confidence'', and ``Breastfed'' are made up of only one question. The construct ``Activities Past 24h'' consists of a count of the number of activities, within the previous 24 hours, that the respondent has done. The construct ``Child Dev. Knowledge'' consists of a series of true/false questions, which are averaged based on whether or not the respondent answered correctly. The rest of the constructs are created by averaging of a set of likert variables.

Descriptive statistics regarding the baseline responses for the outcomes are shown in table \ref{tbl:Outcome Construct Descriptives Pooled Baseline}. Note that many of the constructs have quite high means and medians and some have a high proportion of respondents with the max score. In particular, 73\% and 72\% of respondents scored perfectly on the knowledge questions. This is problematic, as knowledge is often considered the easiest to change quickly and was a core outcome of interest for the team. Additionally, knowledge questions seem to be heavily impacted by the repeated survey effect, as discussed further down.

% Respondents skewed more towards desired responses since we observe large proportions of respondents with desired responses (coded 3 and 4) and very few with "incorrect" responses (coded 1). This pattern is stronger for some variables than others. Variables measuring Responsive Parenting PA have lesser consensus than those measuring Caregiver well-being or Parenting Knowledge. Variables $make\_fun\_of$ and $smile\_around\_child$ each have more than $60\%$ of respondents responding positively.

% We observe ceiling effect for knowledge variables $past\_24h\_play$, $name\_colors$, $know\_name\_age$, $knows\_phys\_dev$ where over 90\% of respondents have marked the correct response. We observe NAs for variables that only some respondents view based on whether the question is applicable to them. Since 28 out of 33 variable have a mean of over 0.5, the responses skew towards being correct/desired response. The Child Development Knowledge variables with more uncertainty are $alphabet$, $say\_name\_age$ and $scribble$. Variables $breastfed$ and $decrease\_stress$ measuring whether the baby was breastfed within the last 24 hours and whether the parent has techniques to decrease stress levels, also have lower means.

\input{descriptives/tables/Outcome Construct Descriptives Pooled Baseline}



% \clearpage

% \section*{Baseline Construct Distributions}

% \includegraphics[width=\textwidth]{plots/Original Data - Knowledge and Awareness.png}
% \includegraphics[width=\textwidth]{plots/Original Data - Practices.png}
% \includegraphics[width=\textwidth]{plots/Original Data - Confidence and Attitudes.png}


% \clearpage

% \section*{Transformed Construct Distributions}
% \includegraphics[width=\textwidth]{plots/Transformed Data.png}


% \clearpage


\addcontentsline{toc}{subsection}{Pre-Exposure to Bebbo}
\subsection*{Pre-Exposure to Bebbo}

This study recruited Serbian and Bulgarian caregivers online, via social media ads, and invited half of them to download the app Bebbo. What if some people were already familiar with the app? Or had already downloaded and used it before?

If someone had already downloaded the app and still had it on their phone, we would not be able to track their usage and they would be considered ``non-compliant'' in this design. This is desireable from an analysis perspective, as these people are ``always-takers'' (\cite{Imbens2015}) who would have the app regardless of whether they were assigned the treatment or control condition.

Many other people, however, might have decided not to download the app because they had heard about it before or tried it out before and deleted it. This is a concern for the study because these people might have already gotten use out of Bebbo: they could have used it and learned everything there is to learn from the app already.

To check for such ``pre-exposure,'' we ask control group users, at the end of the final follow up survey, if they have ever heard of Bebbo or used Bebbo.

55\% of respondents said that they had heard about the app Bebbo and 23\% said that they had downloaded and used the app Bebbo. It's worth noting that there might be some social desireability bias or acquiesence bias (\cite{Stantcheva2023}) in these responses and we do not have a good way to detect that in this instance. However, despite those potential biases, this is strong suggestive evidence that there was pre-exposure to the treatment in our sample.



\addcontentsline{toc}{subsection}{Power Analysis}
\subsection*{Power Analysis}


Post-hoc power analysis was performed to see the ability to detect an effect, in terms of standardized deviations (corresponding to Coen's D effect sizes), in the datasets analyzed. To create the effect size, the standardized different is multiplied by the empircal takeup of 28\%, which was the percentage of participants that had at least one learning event in the treatment group.

The results show that the study is reasonably powered (70\%) to detect a medium effect size on the 28\% takeup at a significance level of 1.25\%, the equivalent of 10\% when controlling for multiple testing (8 outcomes) with a Bonferroni correction. See figure \ref{fig:Power Analysis}.


\addcontentsline{toc}{subsection}{Reliability Analysis}
\subsection*{Reliability Analysis}

The outcomes consist of ``constructs,'' some of which combine the answers to multiple questions into one value. The theory is that these questions are measuring the same underlying construct and that the reliability of the construct is increased by combining multiple answers.

We test this assumption, that they are measuring the same underlying construct, by looking for internal consistency using Chronbach's alpha within the variables associated with each construct. Note that all constructs are composed of either Likert scale variables or Binary scale variables and not both, making this analysis valid.

This technique was used to finalize the construct/variable mapping after the data collection completed but before the analysis began, as some of the constructs had a lower internal consistency than hoped. Table \ref{tbl:Reliability: Alpha Matrix} summarizes raw and standardized alpha of each construct as used in the final analysis, along with the number of variables in it.

Constructs witha reliability above 0.70 are considered internally consistent. All the constructs were modified to ensure higher reliability (sometimes dropping to one variable). The construct created from Activities in the Past 24 hours had the lowest reliability, possibly because some of the activities might be negatively correlated. Because of this, we decided to use the sum of the variables rather than the mean, removing any concern of internal consistency and rendering the low Chronbach's alpha irrelevant.

\input{descriptives/tables/Reliability: Alpha Matrix}


% \subsubsection*{Methodology}
% For every respondent that was treated, i.e., asked to download the Bebbo app, we note the time that they were asked to download the app in the survey. We then aggregate their app usage activity by the weeks since they downloaded the app to get weekly app usage activity for each respondent. Note that this includes respondents across both Serbia and Bulgaria and across both baseline and endline periods.
% We create variables summarizing their activity using events logged in the app.

% \begin{enumerate}
%     \item Home opens - Number of times the home page was opened in the week
%     \item Days used - Number of days in the week the respondents used the app
%     \item Usage count - Number of app events logged for the respondents in the week
%     \item Opened - Binary variable indicating if any \textit{'\_opened'} event was logged in the week
%     \item Home opened - Binary variable indicating if any \textit{'Home\_opened'} event was logged in the week
% \end{enumerate}

% \begin{figure}
% \subsubsection*{Home opens over lifetime}
% \includegraphics[scale=0.4]{app usage/plots/individual aggregates - home opens over lifetime.png}

% \subsubsection*{Number of home opens over time}
% The number of respondents who opened the app drops off steeply after week 1. 744 respondents filled had home open event logged in week 1 since treatment. 129 respondents had home open event logged in week 2.
% \includegraphics[scale=0.4]{app usage/plots/home opens over time - home opens.png}
% \end{figure}

% \begin{figure}
% \subsubsection*{Number of days used}
% We report the mean number of days of the week the respondent used the app. Respondents have app activity on 1 day in the week for most weeks. The text label annotates the number of respondents who opened any page of the app and contribute to the observations for that week.
% \includegraphics[scale=0.35]{app usage/plots/app usage over time - days used.png}

% \subsubsection*{Usage counts}
% We report the mean number of events logged for respondents for each week since treatment. The number of events logged drops steeply after week 1. The text label annotates the number of respondents who opened any page of the app and contributed to the observations for that week.
% \includegraphics[scale=0.35]{app usage/plots/app usage over time - usage count.png}
% \end{figure}

% \begin{figure}
% \subsubsection*{Home opens}
% We report the mean number of home opens per respondent for each week since treatment. By week 4, the majority of respondents do not open the home page of the app except for some outliers. The text label annotates the number of respondents who opened any page of the app and contributed to the observations for that week.
% \includegraphics[scale=0.35]{app usage/plots/app usage over time - home opens.png}
% \end{figure}


\addcontentsline{toc}{subsection}{Attrition \& Survey Behavior}
\subsection*{Attrition \& Survey Behavior}

As an online sudy, attrition was generally high: ~52\% of those who started the survey dropped off before completing it and 54\% never came back from the baseline to complete the endline. Table \ref{tbl:Response Rate by Stage} summarizes attrition by stage and treatment condition. It's worth noting that attrition was consistently higher among the treatment group, possibly related to the increased number of questions in the endline survey for that group (additional questions about app usage were added for the treated).

Attrition was particularly high between endline and follow-up survey (66\%) but that was primarily driven by a problem in the implementation: due to a mistake in survey coding, the questions asked to the control and treatment group was switched at endline, which informed the control group about the existence of the Bebbo app, potentially contaminating them as a pure control. While high pre-existing awareness was discovered in all groups, even those without this mixup, we have removed all cohorts who experienced the mixup from the follow-up survey analysis to avoid any potential issues.

\input{descriptives/tables/Attrition: Pooled.tex}

Note that respondents should have been contacted 4 weeks after each wave in order to take the subsequent wave. However, two factors may lead to them not always started the wave after exactly 4 weeks: (i) there were some technical issues which caused the notification to be delayed in some cases and (ii) not everyone begins the survey immediately when notified and maybe need to be reminded several times, or may remember on their own, significantly later.

To improve consistency of the study, we removed anyone who took the endline or followup surveys more than 9 weeks after their previous survey, ensuring that all respondents were responding in a gap between 4-9 weeks. Table \ref{tbl:Time Gap Descriptives} summarizes the distribution of this time gap. As you can see, the vast majority (more than 80\%) took the survey after 4-5 weeks of the previous survey.

\input{descriptives/tables/Time Gap Descriptives}



\section{Results}

\addcontentsline{toc}{subsection}{Regression Model}
\subsection*{Regression Model}

We run the following regression model to measure the intent-to-treat effect (ITT) of assignment to the treatment arm:
$$
y_{i} - y^{b}_i = \gamma_1 + \beta T_{i} + \gamma_2X_{i} + \epsilon_i
$$

Where $y_i$ represents the outcome of interest for individual $i$ measured after treatment, $T_i$ represents the random treatment assignment, $X_i$ a set of control variables and $y^b_i$ represents the outcome of interest measured before treatment. The parameter of interest will be the treatment effect, $\beta$.

Note that due to the relatively large number of sepearate outcomes (8), we adjust p-values of the treatment variable to control the false discovery rate (FDR), using Benjamini-Hochberg, reported as the ``Adjusted Treatment p-value.''

Of interest in this analysis is also the treatment effect on the treated (ToT), which can be estimated using an instrumental variable model given the monotonicity assumption of treatment (\cite{Imbens2015}), which assumes that people are not less likely to download and use the Bebbo app in the treatment group. To estimate our instrumental variable model, we use 2-stage least squares:
\begin{align*}
y_{i} - y^{b}_i &= \gamma_1 + \beta \hat{z}_{i} + \gamma_2X_{i} + \epsilon_i \\
z_{i} &= \gamma_3 + \gamma_4 T_{i} + \gamma_5X_{i} + \delta_i
\end{align*}

Where $z_i$ is a binary indicator of takeup based on the recorded app-usage data and $\hat{z_i}$ the predicted takeup based on the first stage regression. Once again, parameter of interest is $\beta$. It's worth noting that, given that we cannot say we measured any impact (results are not significant from zero) in the ITT model, the exact values of the ToT model are of less interest and the associated tables can be found in the appendix.

We also run the regression for two separate time periods: endline and follow-up. However, due to large attrition in the follow-up survey and the low long-term app takeup, we are underpowered in our analysis. The results of the follow up regressions can also be found in the appendix.

One of the dangers of a prepost design is that you are priming your respondents with the first survey and that priming may impact how they answer the questions in the post-treatment survey(s) (\cite{Stantcheva2023}). Given this particular study design, where our control is a ``treatment as usual'' (TAU) that involved sharing a website and we do not have data regarding the takeup, or usage, of the website, it is difficult to isolate a priming effect.

We will also plot raw charts showing mean scores at baseline and endline for three groups for each variable: control, treatment with takeup (those who we know downloaded and used the app), treatment without takeup (those for whom we have no data showing they downloaded or used the app). These plots can provide suggestive evidence of priming effects by showing the shift in mean between baseline and endline across all three groups.

\addcontentsline{toc}{subsection}{Knowledge and Awareness}
\subsection*{Knowledge and Awareness}

Regression analysis of these outcome constructs show no significant result of treatment:

\input{regressions/Pooled: OLS - Endline - Knowledge and Awareness.tex}

These two constructs, Vaccine Knowledge and Child Development Knowledge, both suffered from ceiling effects in the baseline survey (72\% and 73\% respectively). On top of those ceiling effets, they both potentially suffered from priming effects, as evidenced by the consistent improvement in the endline survey for all groups.

Note that there is some suggestive evidence that those with less vaccine knowledge were more likely to download the app, indicating that takeup might be biased towards those who need it the most. That might be driving the small and statistically insignificant positive measured impact on Vaccine Knowledge in the regression. Unfortunately, the study was not designed for subgroup analysis on a small group such as the 28\% who failed the vaccine knowledge question at baseline.
\begin{figure}[H]
  \centering
\includegraphics[width=\textwidth]{plots/pre_post/Pooled: Vaccine Knowledge.png}
\end{figure}


\addcontentsline{toc}{subsection}{Confidence and Attitudes}
\subsection*{Confidence and Attitudes}

Attitude Towards Physical punishment is a single question which asks if the parent believes the child needs to be physically punished. While there might seem to be some suggestive evidence from the coefficients of the regression model, the raw data shows that the positive coefficient is indicative of the fact that the control group got worse over time! They were more supportive of phyisical punishment in the endline survey. While there might be a story to that, it could also be the exact kind of statistical anomoly that multiple testing correction is designed to help us avoid when checking so many outcomes.

Parenting Confidence shows no significant impact in the regression analysis. The raw data shows suggestive evidence that those with lower confidence might be more likely to take up the treatment. The lack of a positive coefficient in the regression, however, might indicate that those in the control group were equally likely to take up either the control website or seek out information on their own in order to improve by endline.

\input{regressions/Pooled: OLS - Endline - Confidence and Attitudes.tex}
\begin{figure}[H]
  \centering
\includegraphics[width=\textwidth]{plots/pre_post/Pooled: Parenting Confidence.png}
\end{figure}


\addcontentsline{toc}{subsection}{Practices}
\subsection*{Practices}

These four constructs all relate to practices and behaviors of the parent. No significant effect was found for any of the behaviors and there is not much suggestive evidence of selective takeup either. The raw regression results suggest that Activities Past 24h show suggestive evidence of impact, but the raw data shows that the much of the improvement is driven by those in the treated group who did not takeup the treatment, which gives credence to the assumption that this could be statistical noise and is why we have corrected for multiple testing.

\input{regressions/Pooled: OLS - Endline - Practices.tex}

\begin{figure}[H]
  \centering
\includegraphics[width=0.9\textwidth]{plots/pre_post/Pooled: Breastfed.png}
\includegraphics[width=0.9\textwidth]{plots/pre_post/Pooled: Positive Practices.png}

\end{figure}

\addcontentsline{toc}{subsection}{Policy Implications of the Results}
\subsection*{Policy Implications of the Results}

We do not find any significant effect of the use of Bebbo on any of the outcome constructs of interest.

Three reasons, shown in the descriptive data as well as the raw pre-post data might explain why that is the case:
\begin{enumerate}
\item The presence of ceiling effects, where much of the population scored high in the baseline and could not improve in the endline.
\item Priming effects led to participants improving from the first questionnaire to the second questionnaire, regardless of treatment arm and regardless of compliance.
\item Low app usage. While takeup defined as ``had at least one learning event'' was 28\%, which would be enough to measure impacts, it's reasonable to believe that in order to have an impact on these outcomes, especially behaviors and attitudes, participants would need to use the app continuously. Especially if we consider the advantage of an app over a static website or informational fly, the advantage comes through continued usage (it is available on your home screen, can send you push notifications, etc.). Given that only 3\% used the app more than three days, we would not expect to see much of an impact of this app on the population.
\end{enumerate}

Ceiling effects might be a failure in the creation of the survey instrument. They could also be an example in the bias of the sample population (they are all better-than-average caregivers). But there could be a policy implication as well: it could indicate that most caregivers are quite good already at these outcomes, which is important to consider in the means of addressing the problem. In particular: it could indicate the importance of learning about and focusing effort on subgroups that are worse off. Towards that end, we will perform an analysis to determine the characteristics of the ``worse'' caregivers.

Priming effects are a result of the study design, however, they indicate potential policy implications as well. In particular: if asking people questions (``Do you know which vaccine your child needs to take next'') has such a powerful effect on their knowledge, awareness campaigns might be enough to drive results on these outcomes. Knowledge about vaccines and knowledge about child development both seem like good candidates for such an intervention, given this study.

Finally, low app usage implies that either (i) any app must go through extensive testing and improvement before it will be expected to make an impact measurable on a population level or (ii) apps might not be the most effective method of engaging parents. Like any intervention: the implementation matters and each app can be very different. One app failing to engage does not mean that all apps will fail to engage, however, it does leave the possibility open.



\section{User Characteristics Correlated with App Usage}


\addcontentsline{toc}{subsection}{Analysis Design}
\subsection*{Analysis Design}


In this analysis, we attempt to answer the questions -
\begin{enumerate}
    \item Who are the respondents who used the app?
    \item Do more knowledgeable parents use the app more?
\end{enumerate}

We do so by regressing respondents' app usage activity against their characteristics.
\vspace{1em}

\textbf{Independent variables :}
\begin{itemize}
    \item \textbf{Baseline characteristics} - respondents' scores on the construct variables. We only use the construct variables that have sufficiently high internal consistency. We drop the constructs that are highly correlated with other constructs. We find $parent\_knw$ to be correlated with $caregiver\_well\_being$. We drop the construct with lower reliability $caregiver\_well\_being$.
    \item Demographics variables - parents' age flag (categorical), university flag (binary), gender (categorical), and number of children (numeric)
    \item Survey response variables - survey duration, start week, country flag
\end{itemize}

\textbf{Dependent variables :}
\begin{itemize}
    \item Home opened - binary variable indicating whether the respondent had a home opened event logged
    \item Home opens - continuous count variable indicating the respondents' count of total home opens
\end{itemize}

\addcontentsline{toc}{subsection}{Download Analysis}
\subsection*{Download Analysis}

For respondents who were treated, i.e., asked to download the Bebbo app, we fit a logistic regression model to whether the respondent had a home opened event logged using the respondents' baseline characteristics, demographic variables, and their survey response variables (Table \ref{tbl:downloads and baseline characteristics - logistic reg}). The goodness of fit measure used is the percentage improvement in deviance over the null deviance (pseudo $R^{2}$). The pseudo $R^{2}$ for this model is 0.03.
\input{app usage/tables/home opened and baseline characteristics - logistic reg}


\addcontentsline{toc}{subsection}{Continued Usage Analysis}
\subsection*{Continued Usage Analysis}


For respondents who downloaded the app, we regress their number of home opens against their baseline characteristics, demographic variables, and survey response variables. The $R^{2}$ for this model is 0.065. Note that the country flag is not significant which means that app usage does not differ significantly across the two countries after holding constant user characteristics (Table \ref{tbl:app usage and baseline characteristics - linear reg}).
\input{app usage/tables/app usage and baseline characteristics - linear reg}



\section{Conclusions and Reccomendations}

% \addcontentsline{toc}{subsection}{Analysis Design}
% \subsection*{Analysis Design}


\printbibliography

\appendix

\section{Baseline Balance}

To test for balance between our randomly assigned treatment and control groups, we run an omnibus test, following Hansen and Bowers (2008), to observe standardized differences at baseline and the associated omnibus p-value. Results are reported separately for each country and found in tables \ref{tbl:Baseline Balance Serbia} and \ref{tbl:Baseline Balance Bulgaria}. Following \cite{Altman2014}, we do not change our analysis plan based on these results, but it is worth noting that the Bulgaria data does seem to suffer from slight unusual differences between treatment and control condition and the p-value of the omnibus test is significantly low. All the analysis is also reported for only those respondents in Serbia as well, which serves as a robustness check against any concerns that Bulgarians were randomizes into unlucky groups for our analysis.


% \input{balance/Baseline Balance Pooled}
\input{balance/Baseline Balance Serbia}
\input{balance/Baseline Balance Bulgaria}

\section{Additional Plots}

\begin{figure}[H]
\begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[scale=0.33]{descriptives/plots/correlations_constructs_Serbia_Baseline.jpg}
    \caption{Construct Correlations - Serbia}
    \label{fig:serbia correlations}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \includegraphics[scale=0.33]{descriptives/plots/correlations_constructs_Bulgaria_Baseline.jpg}
    \caption{Construct Correlations - Bulgaria}
    \label{fig:bulgaria correlations}
\end{minipage}%
\end{figure}

\begin{figure}[H]
  \includegraphics[width=1.0\textwidth]{plots/Power Calculations.png}
  \caption{Power Analysis at 28\% Takeup}
  \label{fig:Power Analysis}
\end{figure}

\clearpage


\begin{figure}[H]
\includegraphics[width=\textwidth]{plots/Adjusted Coefficient Plot Knowledge and Attitudes.png}
\includegraphics[width=\textwidth]{plots/Adjusted Coefficient Plot Practices.png}
\caption{Adjusted Coefficient Plots of 2SLS in Pooled Dataset}
\end{figure}

\section{Additional Tables}

\input{descriptives/tables/Outcome Construct Descriptives Serbia Baseline}
\input{descriptives/tables/Outcome Construct Descriptives Bulgaria Baseline}

\input{descriptives/tables/Attrition: Serbia.tex}
\input{descriptives/tables/Attrition: Bulgaria.tex}

\section{Additional Regressions}



\input{regressions/Pooled: 2SLS - Endline - Knowledge and Awareness.tex}
\input{regressions/Pooled: 2SLS - Endline - Confidence and Attitudes.tex}
\input{regressions/Pooled: 2SLS - Endline - Practices.tex}

\input{regressions/Serbia: OLS - Endline - Knowledge and Awareness.tex}
\begin{figure}[H]
  \centering
\includegraphics[width=0.9\textwidth]{plots/pre_post/Serbia: Vaccine Knowledge.png}
\end{figure}


\input{regressions/Serbia: OLS - Endline - Confidence and Attitudes.tex}
\begin{figure}[H]
  \centering
\includegraphics[width=0.9\textwidth]{plots/pre_post/Serbia: Parenting Confidence.png}
\end{figure}


\input{regressions/Serbia: OLS - Endline - Practices.tex}
\begin{figure}[H]
  \centering
\includegraphics[width=0.9\textwidth]{plots/pre_post/Serbia: Breastfed.png}
\includegraphics[width=0.9\textwidth]{plots/pre_post/Serbia: Positive Practices.png}
\end{figure}


\input{regressions/Serbia: 2SLS - Endline - Knowledge and Awareness.tex}
\input{regressions/Serbia: 2SLS - Endline - Confidence and Attitudes.tex}
\input{regressions/Serbia: 2SLS - Endline - Practices.tex}

\input{regressions/Pooled for Follow Up: OLS - Follow Up - Knowledge and Awareness.tex}
\input{regressions/Pooled for Follow Up: OLS - Follow Up - Confidence and Attitudes.tex}
\input{regressions/Pooled for Follow Up: OLS - Follow Up - Practices.tex}

\input{regressions/Pooled for Follow Up: 2SLS - Follow Up - Knowledge and Awareness.tex}
\input{regressions/Pooled for Follow Up: 2SLS - Follow Up - Confidence and Attitudes.tex}
\input{regressions/Pooled for Follow Up: 2SLS - Follow Up - Practices.tex}


% \input{regressions/Bulgaria: OLS - Endline - Knowledge and Awareness.tex}
% \input{regressions/Bulgaria: OLS - Endline - Confidence and Attitudes.tex}
% \input{regressions/Bulgaria: OLS - Endline - Practices.tex}

% \input{regressions/Bulgaria: 2SLS - Endline - Knowledge and Awareness.tex}
% \input{regressions/Bulgaria: 2SLS - Endline - Confidence and Attitudes.tex}
% \input{regressions/Bulgaria: 2SLS - Endline - Practices.tex}


% \input{regressions/Serbia: OLS - Follow Up - Knowledge and Awareness.tex}
% \input{regressions/Serbia: OLS - Follow Up - Confidence and Attitudes.tex}
% \input{regressions/Serbia: OLS - Follow Up - Practices.tex}

% \input{regressions/Serbia: 2SLS - Follow Up - Knowledge and Awareness.tex}
% \input{regressions/Serbia: 2SLS - Follow Up - Confidence and Attitudes.tex}
% \input{regressions/Serbia: 2SLS - Follow Up - Practices.tex}

% \input{regressions/Bulgaria: OLS - Follow Up - Knowledge and Awareness.tex}
% \input{regressions/Bulgaria: OLS - Follow Up - Confidence and Attitudes.tex}
% \input{regressions/Bulgaria: OLS - Follow Up - Practices.tex}

% \input{regressions/Bulgaria: 2SLS - Follow Up - Knowledge and Awareness.tex}
% \input{regressions/Bulgaria: 2SLS - Follow Up - Confidence and Attitudes.tex}
% \input{regressions/Bulgaria: 2SLS - Follow Up - Practices.tex}


\section{Survey Instrument}

\input{descriptives/tables/Construct Variable Mapping}

\end{document}